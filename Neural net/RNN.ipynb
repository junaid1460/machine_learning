{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from __future__ import print_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data has 1125 characters, 56 unique.\n",
      "----\n",
      " so?a?PwRdATxP-oYvOG(Lqs;M(t,,OqgAcgPDSkLnNPN\n",
      "Hh)ap\n",
      "'!SaNNrCWGkalQNAhIoAhd./;I .WW/CL(wtwHcv?CavHD.PnH\n",
      "-lgR)w HpoTw;beSPsixOIBuxPMAC-hL.'B)MAecPYAbcbxfpBpmT!TGOB)vtt!WQbgTk +BALdh\".PrrMG'b+L.lnfwcG' NC \n",
      "----\n",
      "iter 0, loss: 100.633784\n",
      "----\n",
      " su ,ckem P lfur o,eDe eReueseW;l !ed ,eh iohe/CieradffdRePlOefrhtM'bhoe\n",
      "gy?teyoyoveulumNes+i ,arecg,S s ?tm,eYha/uwHsPAiueQh ;'uyvlu?veviteumhtrmnerhieeDioi u?r\"d ioyastWg.eoegAv (eloketsoenmvel kso u \n",
      "----\n",
      "iter 100, loss: 101.840896\n",
      "----\n",
      "  d i o waoraricolO!syn Rmppir r pta  frg smneolsl  ilsmblolstors)g ilneimr oenp maaoei ohalae Go olinoairnrl iSWpsloeoGopottmopt s oeadslt ulsymhs' ngpnngrnwce nilianrDsseein oeoeP slepielefenre Pril  \n",
      "----\n",
      "iter 200, loss: 100.073025\n",
      "----\n",
      " .?Beioan.lo wh cc lhen or w' Li \"o.oCYt c dl daeu .osuttc a de  qo?ua'eeiyafe loa c luae lt aoiedue a iuener outro  sos tot teo aoP toacee d Houldstsotoehlrsta due a OvPsahrouhan m.c o vt iotofnale do \n",
      "----\n",
      "iter 300, loss: 98.035339\n",
      "----\n",
      " rtade ate gmenMnir SNtCn g pfvei  Wr ttv,uenf dsxnGretolite.dd ee yohelmldum lotulalfeerit.nHp ls -tO\n",
      "\"cencyoi;let'ewlart inxmelvndM tyu a tOcnnane \"ngn tinwietBi m.m.oalelchengritenllfl. ,ra ps n,een \n",
      "----\n",
      "iter 400, loss: 95.597521\n",
      "----\n",
      " ?tratisevatvsand isn HresnCfdoret.CWIfr BemeOid in-lroagaa f c,tgatorist i?ne, thiel n ftit pc\"vensf?oan. snate snso o v i?r innn ddol palu ivs vit -nu aNebtedet on tsean LS+  Ge,ara do  th; etveo al  \n",
      "----\n",
      "iter 500, loss: 92.945112\n",
      "----\n",
      "  ilf tomrmting. .nurmpppesscht oof b ong Lvor inng.fhse at.gh fom Now sumphi Dhpelu hutordhipWs or Hhes iriavveduowibvanntatots'mteteaSt kiQ Bir am we.l shes voesoshe ougigr  g/o(  akti,s de ihiryht-  \n",
      "----\n",
      "iter 600, loss: 90.098676\n",
      "----\n",
      " cng. otu ics re? Beves te chemings ind. crtece pretome cot alng lo ta an sooug Sf PN/QPr inl of arteS lIis trere seripmt pfgisle to ing, ntoo as g is'ewit tivres cor ceveat. as gosmeveil, cor'edad apm \n",
      "----\n",
      "iter 700, loss: 87.242393\n",
      "----\n",
      " veis crho p SHeve nh pef fing Od? CQur WToud pe gourly our ps pofmeherdat aat iel \"ormtovel fo? gfelsri gf Ipr Win 'esinnf ateOSerecrimeangat ron ufmour ipu fingp (o \"re if ingaateonlrtadilen Or\"our a \n",
      "----\n",
      "iter 800, loss: 84.451722\n",
      "----\n",
      " uiml'l'r \n",
      "Nomule  logil gort, CLGevicato prlant Wourlth. ndef ffeper. Thindis it ye wel'yo WPe, ,oos, av te Ar toatlps Rn'u touvOl\"tr tr yol Be dovillx\n",
      "? Thee dred gsentk-in ?reand ang amedodibg OSDep \n",
      "----\n",
      "iter 900, loss: 81.685841\n",
      "----\n",
      " eg ther ecuant  fobut ave nm tou lo n u yos xhea. ?+. ke dhingu ong ark ilu kang SoW Qwim theantd youur tar Are tois,  fose den thteriHellchrems to fyvef ing.carecde nek ir coou yo ?CyoWe wevaprite gh \n",
      "----\n",
      "iter 1000, loss: 79.042981\n",
      "----\n",
      " endoolmasixn iS, ant metee.ntionparte.\n",
      "Burtmenimececthevt angGeas youm at ote Acteortuplec\"mfmmiend glo. MCouut oad fmen aRG youps nte gote premecuilg inmiprite ad ta. Ind Courd ay mendithes abutang t \n",
      "----\n",
      "iter 1100, loss: 76.457896\n",
      "----\n",
      " nth pilupilrt -o as iog'ig prevelachet in g of, coCLfangmme nt? pof primer thevhings oreate mteagsop se ?hecs ad i? yor thr arO amte tivg ebuter.ageyhe SDen m? buelf,.crsilt Ardtat gr che Wen Wenn you \n",
      "----\n",
      "iter 1200, loss: 73.880857\n",
      "----\n",
      " afexkewsepril s andewinn arceit, orol dar af ie', OS. were.chitsinpis arathipils tofWe, anMipngils mripadt is .chiki, romdef. \"P' ?pore end tPofr an ambonpiptevelinGndeand, this erotellil,. olws foosu \n",
      "----\n",
      "iter 1300, loss: 71.388050\n",
      "----\n",
      " dodereoaOrwen at falgolufr onrind thong avdo .qo.ud irs artothet oto pifiwe de th alBu onu'te Anw you Mre AL oulIhe fraond t? rethibstCoass mekate sutme ftot thinur gceCh\"es pendat in yoruteeg thalsut \n",
      "----\n",
      "iter 1400, loss: 68.943551\n",
      "----\n",
      " rutoo you \"helog thisle Woat rotiset oer foiss comsier ton is deen fots youug ortut ilog it e Oc. Wouug Srefen ompgHrhe tferelveriith coour pocler ecox in peGso. Ort \n",
      "ruas ive sand tfilg aSod eretiati \n",
      "----\n",
      "iter 1500, loss: 66.659403\n",
      "----\n",
      "  it tour mengnmet -nnecl, ver handbipte goallls doels noumpiSdo the do psere;g ler ther fbe pit, ire af tar freet. Bu ss.yomes fres (eulm ing f onllructe. btise soaf OSCCureg pinglang, ered Wre Spreev \n",
      "----\n",
      "iter 1600, loss: 64.355837\n",
      "----\n",
      "  feedledeced thes' trops y; ar? you OSIllo Dar the nyo rrate fhergtote bentyoop'inl weengthis croeved i(tyu Ores mens.\n",
      "WOS sabet, .ru all \"oh pero thile e. Obu loRyu crecceate nr ant Loul aHat Loily y \n",
      "----\n",
      "iter 1700, loss: 62.193518\n",
      "----\n",
      " tmedantPAr ate youd inguSed yourenam nat/ieced cango gerhatiir to tse, ofras matis it catied meore etheh ingith? Orons frotongosart diend ar yke ofops ort foshend? prethens afmirlt  Suis Los wonfschiH \n",
      "----\n",
      "iter 1800, loss: 60.120718\n",
      "----\n",
      " ? OR\"n ingOSertend art at gutog  your eromalled hanpello p.amet Lot  aluw \n",
      "our gomefe Anisuirg youO \"honle, Hroisptie d, -C+ shugrecef tevecede\n",
      "t(e grete innd cent ors to th lslings oto De siing irial \n",
      "----\n",
      "iter 1900, loss: 58.032338\n",
      "----\n",
      " heat ollg anmid Cou Thel pramm ntmirertin your to tor and tereant nr, mratiplins thiplloamreas or OS? Wanguthorudr yardacte ceo amd sinethi fig ind weld o. wet youu. If if arts. Nonsu, this Bobeu mand \n",
      "----\n",
      "iter 2000, loss: 56.120077\n",
      "----\n",
      " l dhofwsinpilll'n thengreantveli( ugrinas tover if is you res wou lvvilnu rout yo ngois caml ocaadid grindilve iku seate oe phimmHipland tCongalond Oolve anqwuthereot por ar ade ,out  nonuOSDevp out s \n",
      "----\n",
      "iter 2100, loss: 54.278927\n",
      "----\n",
      " elcommint popillont Bu thet enre tore the prerts inl CMot go you pratotippertot in'ime fon ferobeatdect-ing tfe cheten threang at to eo OSDevelLinured at Ciks andt thertat pref promig? fekemminnddente \n",
      "----\n",
      "iter 2200, loss: 52.527578\n",
      "----\n",
      " ngethot ilg mmmmisumserter ff perliendingHtino? \"rad? But poulls bot alanguseven thev'eng thel your goals, butd lant ant; gf cleweolus orls if mshiu? wre develonparllmento pen Gt/in youraliat, mats wa \n",
      "----\n",
      "iter 2300, loss: 50.746725\n",
      "----\n",
      " Angthore ,\"manis feroceld toru greeve co subet, ang any R//M?W.\n",
      "?Wabe dd ip'llln ufields ces. Be this you ws inly yHarshkis pryours un oy palWiwinl- ureds pingurnet deothveping OSDev'ing akipilll, fat \n",
      "----\n",
      "iter 2400, loss: 49.017964\n",
      "----\n",
      " re. Wmmilended tour lhabut theven in ss ney + alla yo Oad, GCom ads its corecde ome cat-terotit commpmincamotis doat  Sbe tho samen to OSDevl'ingWansindintot ar ther abo ade lo -CQi nd tour weigfad in \n",
      "----\n",
      "iter 2500, loss: 47.440256\n",
      "----\n",
      " as preane dalu an is this akentaen fopraemiclqu Smecete icrsecens grefere tis an avee ocesshelruimengthien-then Cwens Nof ar this ff reu the ne greaed fopssenedey mellr moruand tone But gotiel, Ore te \n",
      "----\n",
      "iter 2600, loss: 45.904672\n",
      "----\n",
      "  ngeven in Av'inou Bde, if ysumhwifr tor grefmenmen Cfare, thinp only unes.\n",
      "C(C.-her at ever LingWare. Was yon psonut Somp yovu gorpengthing tour Ik Gotwerexceten yovruding Scee, mre tor atid elon? Be \n",
      "----\n",
      "iter 2700, loss: 44.396994\n",
      "----\n",
      "  L ang inn tour south in y oals you als indsabeadd thit ang aro prefelel fand. geilld absum pills abk the nuis Linu fres yo alw, but, groillg abeutn cher tor build  no ures. Buit on youlal Hellan  fec \n",
      "----\n",
      "iter 2800, loss: 43.045296\n",
      "----\n",
      " tde.ld  ame ied toreasotheteco fread. In s.OS+ sseite the yo uref is ofter comiel, to as ateise ruind ir fert- ing. But geant gooncu on Won, cshive yougr ufe lonsWoisurte torisicvee tod ass catheelean \n",
      "----\n",
      "iter 2900, loss: 41.757455\n",
      "----\n",
      "  per t\"on urhangecee  Cux? Oreit ce Ct\"on ar d\" go uarent\n",
      "Non peloC aDer IfuOSDer youred oty k\"ent, prtaecd, butewecte  our Wompititin\" Wit, yoc. Ad theed thillom you ured to suie', chammintilg is tha \n",
      "----\n",
      "iter 3000, loss: 40.449951\n",
      "----\n",
      " ertert is OS? Ore\n",
      ". If theng thoels ofapsiel, ibu sems towus orr bit are  \"buu lor -Come. Cw falsu Whathered piver tou preammenows it. bess Nssuth pricg aof wate Wonl pingutheven burucameveptimmibut t \n",
      "----\n",
      "iter 3100, loss: 39.233745\n",
      "----\n",
      " g ping Yhour this dopsehatsipi;mendit ct\"oise, Phevewit iill aof ysumpimmint. IQs ard ed youx dereot your Buis cesCPNout olan you. But yoyu Bees Lat? shing OQlr aRd in ce teog are dQis posuare de Co.  \n",
      "----\n",
      "iter 3200, loss: 38.027416\n",
      "----\n",
      " elcome ing your gonlang ts you sert and the \"Hve'n contilod aro cat, chipscat is Wateve fit in Iwo fessutrentiver) rothels Lant, tor sils on somereed in OSDevs? IRQs aro then lons, but. Ands frien fer \n",
      "----\n",
      "iter 3300, loss: 36.947373\n",
      "----\n",
      " nt? Orut, fomsit cle oake, ore fring ane dehampillg you es.\n",
      "Noxt ar CoQlu Thacle tome go nu suxce don ar eadir dat Wo frecte f OSDev'ing. CHs nre. Wavl, OSDev'ing? greseet do rever yo but geaned eftem \n",
      "----\n",
      "iter 3400, loss: 35.813496\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----\n",
      " duts th ing ag\"ire sour'ingteven thons this dfe gCCM/Hvel, (n the go thilug requtor the grecne oft, ccottive'orepeng. Inds OSDel'e otheven OSDeve forut go fes but ver i; d, mhitvevenping il win OSDev' \n",
      "----\n",
      "iter 3500, loss: 34.619082\n",
      "----\n",
      " lo ofmilor an CCP++ atimAnte tur Icxt ate tor youO Youx OSDev'nexg tore Bu prefer it OSDev'incat Lrot ate S? Olonguare, in IRQs ant progrthog yon arsit cont pricpsifr pirt. nes. Be andiperes abt tevea \n",
      "----\n",
      "iter 3600, loss: 33.637313\n",
      "----\n",
      " on are d\"inp you age fo Ohhtiping ereth ps? OSDtelominitimentod ir ind Skilpintit eated ehert evlin ing reade the \"Hvllanguixl in OSMeentecet to af ar pinu \n",
      " \"hing we\"meniat , Sresite le  CuOxDev'ingu \n",
      "----\n",
      "iter 3700, loss: 32.583844\n",
      "----\n",
      " \"ogrseth inn iel foind fis madlante gophall, our ef, mad teng pimlvl\"ommmminuthere f. But geant dmennter th sef so Lass the grofregermammiileve, our yy Mang ato us co fucce thinls, or If ine atid OSDe \n",
      "----\n",
      "iter 3800, loss: 31.568570\n",
      "----\n",
      "  but frered \"ed fikin th, are to eacsedo fminr ferre to bou and fried the gower iclero fure ss bu In Wur frese nf prile dan in Sudd pon are dean; .ng.wert fopsaxt\n",
      " if you prewemmpiptire, to se case de \n",
      "----\n",
      "iter 3900, loss: 30.788523\n",
      "----\n",
      " nnuthilp ath ing tho heveo mamiiled, thate oftepenthe catid in OSDev'ingu log fro buit ere? Srladd athis ro ansucom uLlpryommeloieddisitretemeame tor lo ofur qur theve nos if ire atew ir yoouage tooua \n",
      "----\n",
      "iter 4000, loss: 29.737888\n",
      "----\n",
      "  (even you reeceiten to ush promasit inuWipmt er you ald on swictsethi\"d ing..\n",
      "WM) no gady your goas ssend ate, actheng torlag to Os- checamient; the great et ux/? Or is ce. ure Gof prograng thupredmi \n",
      "----\n",
      "iter 4100, loss: 28.773611\n",
      "----\n",
      " ie lCo Mheng. Thermen al your greatevang Ssedit inM? pro seveon youDavelyour yf anuxg erotalng OSDeve SHerl, ofrs nexquced thilly and il ore fsilr you soore sf CCC++ simewind ing SCart aGt, sorter ind \n",
      "----\n",
      "iter 4200, loss: 27.900322\n",
      "----\n",
      " g wing paym ind aro utecre, fortane devirit ela ever to OSDev'ingxheremimmit. abe delok iny CYonutheeg to us icle, you seowt prilm fere toconlare the seograming feships botu rea y. wisussind an ere ot \n",
      "----\n",
      "iter 4300, loss: 27.037944\n",
      "----\n",
      " elcome to O//CC/Cf, canuredd ath in ant P/\"(hat, it ive loouOllongeale or cutyomr) fo lad the nuthe goter if Lsamteredt on you rome to OS e, fort or shen cated in ?Cour soit ir uthe great pr you you w \n",
      "----\n",
      "iter 4400, loss: 26.340171\n",
      "----\n",
      " nguag cho faild aning ar chen utement, but psolp you seat. camOcateonguage ien ferheven, mhsiyluronuillag iks or sind eradhilp you preat de grementhen fer fress- Bd thecete  to sucadivellon ang den ic \n",
      "----\n",
      "iter 4500, loss: 25.600299\n",
      "----\n",
      " mendate Oo/M?\n",
      "Whared thipplacex dillmereate geabite tod fo OSDer madmiGem domillripmis, nepmsotipminetdo cOS? arsten bulug socly CaMta. In fathen yo bue, Worup yu .\"/C-Chacet you crameccete G? pronuis \n",
      "----\n",
      "iter 4600, loss: 24.700931\n",
      "----\n",
      " forul gmant,  ur ykonlureont- OS is Wot an uhherweendut onu ans posuthe geate the go fu le shat fe nut, inysu. In C/Mhevamincitedefortact tod along wing ane os poluls ot shing SGesicn if Wor yo uhapin \n",
      "----\n",
      "iter 4700, loss: 23.848420\n",
      "----\n",
      " f ar your durher and are diing fols you are irutwit - an shind ang if OSDev'ing.St went ant din youurllan ure cext; se. nous s? buiareceide ca thalo  frecre to Sut lo Wart amqsiles ne. I+ Cfe great an \n",
      "----\n",
      "iter 4800, loss: 23.190568\n",
      "----\n",
      "  or pilcre softha ever Lang aed do low you  Couraly ant, mOS? Or yonucte ip bku Or. But doprlan yo WhMenmitort cagy preike sots pills mherts this to hus thit Mo nel your pikillyo batt, ind? greate, ab \n",
      "----\n",
      "iter 4900, loss: 22.554671\n",
      "----\n",
      "  StapssOSDev'ing. Be t;ore foisut any ale tout, abut siivveonevemminext acte sotips, buxt rtends ber dor sit otewecefit en'? Sherete Get friar. Bum you'rlle to the dor geatevegomping sebitpill\" ane de \n",
      "----\n",
      "iter 5000, loss: 22.160843\n",
      "----\n",
      " ctitin? Orrer.\n",
      "Wompming. BuLield this ftonu go ffilrented in bous profellop Cous OS? grememtibutwene. du, pongusheq it thesent, Obuucllm toosuilgm, if OS developminnt abe. gofre caG to thvelloru. Buis \n",
      "----\n",
      "iter 5100, loss: 21.632144\n",
      "----\n",
      " x't ing?ymAnd treweth inyu. Bersite lon ureis if ie .; or hf ys-Oqseacet or. felland atho ucls ferg. but, Onpit eofurogratin fqurieddforts bit? Or arkenita tHelloping. This yhuthing if ix tre an this  \n",
      "----\n",
      "iter 5200, loss: 20.940454\n",
      "----\n",
      " sin aver ofting'St onlanmced s yuWhily ouag an ips are sorlam) or cakipssopilllo Iforlland; bsuchice, abut Denw ar ike toibuilden Compilf ing weec to ps irlyopilldo ofres your goals lower pit ere tode \n",
      "----\n",
      "iter 5300, loss: 20.209241\n",
      "----\n",
      " d abu? You prefter at cho rame loonlu isy rhinlo OSDev, prewes tort ir Woflane. gakillleneCtOv\"entograne. In hatinuOSDat prommpit is yocus skis acotipseate OS res.\n",
      "Nout, ores to lan're eeddeved onu ar \n",
      "----\n",
      "iter 5400, loss: 19.509511\n",
      "----\n",
      " elcome to Oveven'ip the \"Gete topile Guteves yofud ouls fore ffows ping. Yous gmantise or. gass-the GHr lang. gresefired memrite, Cous's demsit inendOSHctiing and thengitdeven C/MO? On reat do is ind  \n",
      "----\n",
      "iter 5500, loss: 18.991219\n",
      "----\n",
      " ng wsing)wer. ar pislare toog sores youth lang ale ofthreat in ouaeres ping sreven pas are your poferene .ch abs thes youla gorlyou ares you wiil \"oorly your ind ir deol foonus you prete to theveresim \n",
      "----\n",
      "iter 5600, loss: 18.494489\n",
      "----\n",
      " yof no ihisenecte. nut you p?omtentods your gored in futhe glon an Whelon you'relerind art pinguOSDev'iing are diven ore friendships OSDev'ir'ildon in ghune s\"amsinls (efires fellad rentonlad ir abues \n",
      "----\n",
      "iter 5700, loss: 17.965538\n",
      "----\n",
      " noming Ske, you're fe nt; the next LingxOSDev'ingutheveno /Mu'n soures ferrtom ant, prous youurelenow MeGt, if OS ere tevelemm\"mindti pfrier.camimiilyo wonusyou. But ir this cat filnuxchieg irm crefer \n",
      "----\n",
      "iter 5800, loss: 17.405446\n",
      "----\n",
      " f loour ant an  But gothemanit ore, our gferot ps fel ure red tonl your lu ise wemedde gomadte geent siping. botills your, fatyou grefr pnus? Wad ale Syof lrenled orls - Mece ding anour profello'so .w \n",
      "----\n",
      "iter 5900, loss: 17.105511\n",
      "----\n",
      "  ot onlang ane deag ise.g too ance.d abtuds ind dee \"amicf ing Simed at \"s it is Worucoteveat pinis coitu lagusiend the gractimLinltwen end wiene. \n",
      "Cxbutientat chmimmniter) or tfeygmant ping ammine. W \n",
      "----\n",
      "iter 6000, loss: 16.655411\n",
      "----\n",
      "  friendshis Win ucrese. Io long any d or.\n",
      "Not all youuren abuu rea. In fate p Steven SCuria sided to son ise ead thind ared to s? Or is mommunte geant ily ad you re fatee gattexld toriang acl, bucts i \n",
      "----\n",
      "iter 6100, loss: 16.088724\n",
      "----\n",
      " n herento OSDee doting freat thing tofu Be thoshlan tLinpit even'? nht  thie, the clkigrtin ? OSDes fortag. Bot yop'is ort frifr IRut lo OSDev'ing. Thellerefmerte to psin thes Wfongyend Cound not us y \n",
      "----\n",
      "iter 6200, loss: 15.529053\n",
      "----\n",
      " xt Linux? Or Winsuane devsingtereend are+ eare. In  uriibuild in\" ake you grait enwev'ingit lo you  len't gexce inuxw cad the gSend's youOS Or ere thonls pis ure ded, and at fix? Os coour go ure Leng  \n",
      "----\n",
      "iter 6300, loss: 14.917944\n",
      "----\n",
      " imHillld our lor tools outing wen ConDev'inguare gean tPonuat totis gie lac, teref ysu. dacele to OPnuxceene mimpistheng. This iven torled orlan your, abe. Builded ibut orewece to ss it even oulag pos \n",
      "----\n",
      "iter 6400, loss: 14.443312\n",
      "----\n",
      " d ake the neven Cou's lo OSDes i? OSDell opile tConullsanthell ort is ir. anl, mate devene.mpisto Ogael youOwent priendswie, andwing skieldo ffis ate ter tien fo griamt is abothaly buted oridureog as, \n",
      "----\n",
      "iter 6500, loss: 13.927730\n",
      "----\n",
      " elcome tomhen'illo OSDev'ing. This are deothe gmement omu ir IRQs inuthereed ot an chefereme bous lou at is cat tho this it the \"He lo nutever momill dower if sime lad are toe language Windave'eng ar  \n",
      "----\n",
      "iter 6600, loss: 13.478832\n",
      "----\n",
      " ng is tho shens otheng.ched is ofter your goals lower -PMendedeicntit ionguare or youOg ar ssont create the greate toou are ate the next Linuxs en evanm inl, your suthhe not ant \"GDevle pmkiersing. In \n",
      "----\n",
      "iter 6700, loss: 13.069909\n",
      "----\n",
      " member abe gfang ssene tomsante ker ticextpillonwfrad soour fie If psecedd il forums and this Wikicllon and devele of ir Woll, next fingripinn tke come toduis are teo furte fC freale referre to psing  \n",
      "----\n",
      "iter 6800, loss: 12.825673\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-9f0a549330eb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m     \u001b[0;31m# forward seq_length characters through the net and fetch gradient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m     \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdWxh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdWhh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdWhy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdbh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdby\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhprev\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlossFun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhprev\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     98\u001b[0m     \u001b[0msmooth_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msmooth_loss\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m0.999\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m0.001\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mn\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m100\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'iter %d, loss: %f'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msmooth_loss\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;31m# print progress\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-16-9f0a549330eb>\u001b[0m in \u001b[0;36mlossFun\u001b[0;34m(inputs, targets, hprev)\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0mdWxh\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdhraw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0mdWhh\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdhraw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m         \u001b[0mdhnext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mWhh\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdhraw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mdparam\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdWxh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdWhh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdWhy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdbh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdby\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdparam\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# clip to mitigate exploding gradients\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# data I/O\n",
    "data = open('input.txt', 'r').read() # should be simple plain text file\n",
    "chars = list(set(data))\n",
    "data_size, vocab_size = len(data), len(chars)\n",
    "print('data has %d characters, %d unique.' % (data_size, vocab_size))\n",
    "char_to_ix = { ch:i for i,ch in enumerate(chars) }\n",
    "ix_to_char = { i:ch for i,ch in enumerate(chars) }\n",
    "\n",
    "# hyperparameters\n",
    "hidden_size = 100 # size of hidden layer of neurons\n",
    "seq_length = 25 # number of steps to unroll the RNN for\n",
    "learning_rate = 1e-1\n",
    "\n",
    "# model parameters\n",
    "Wxh = np.random.randn(hidden_size, vocab_size)*0.01 # input to hidden\n",
    "Whh = np.random.randn(hidden_size, hidden_size)*0.01 # hidden to hidden\n",
    "Why = np.random.randn(vocab_size, hidden_size)*0.01 # hidden to output\n",
    "bh = np.zeros((hidden_size, 1)) # hidden bias\n",
    "by = np.zeros((vocab_size, 1)) # output bias\n",
    "\n",
    "def lossFun(inputs, targets, hprev):\n",
    "    \"\"\"\n",
    "    inputs,targets are both list of integers.\n",
    "    hprev is Hx1 array of initial hidden state\n",
    "    returns the loss, gradients on model parameters, and last hidden state\n",
    "    \"\"\"\n",
    "    xs, hs, ys, ps = {}, {}, {}, {}\n",
    "    hs[-1] = np.copy(hprev)\n",
    "    loss = 0\n",
    "    # forward pass\n",
    "    for t in range(len(inputs)):\n",
    "        xs[t] = np.zeros((vocab_size,1)) # encode in 1-of-k representation\n",
    "        xs[t][inputs[t]] = 1\n",
    "        hs[t] = np.tanh(np.dot(Wxh, xs[t]) + np.dot(Whh, hs[t-1]) + bh) # hidden state\n",
    "        ys[t] = np.dot(Why, hs[t]) + by # unnormalized log probabilities for next chars\n",
    "        ps[t] = np.exp(ys[t]) / np.sum(np.exp(ys[t])) # probabilities for next chars\n",
    "        loss += -np.log(ps[t][targets[t],0]) # softmax (cross-entropy loss)\n",
    "    # backward pass: compute gradients going backwards\n",
    "    dWxh, dWhh, dWhy = np.zeros_like(Wxh), np.zeros_like(Whh), np.zeros_like(Why)\n",
    "    dbh, dby = np.zeros_like(bh), np.zeros_like(by)\n",
    "    dhnext = np.zeros_like(hs[0])\n",
    "    for t in reversed(range(len(inputs))):\n",
    "        dy = np.copy(ps[t])\n",
    "        dy[targets[t]] -= 1 # backprop into y. see http://cs231n.github.io/neural-networks-case-study/#grad if confused here\n",
    "        dWhy += np.dot(dy, hs[t].T)\n",
    "        dby += dy\n",
    "        dh = np.dot(Why.T, dy) + dhnext # backprop into h\n",
    "        dhraw = (1 - hs[t] * hs[t]) * dh # backprop through tanh nonlinearity\n",
    "        dbh += dhraw\n",
    "        dWxh += np.dot(dhraw, xs[t].T)\n",
    "        dWhh += np.dot(dhraw, hs[t-1].T)\n",
    "        dhnext = np.dot(Whh.T, dhraw)\n",
    "    for dparam in [dWxh, dWhh, dWhy, dbh, dby]:\n",
    "        np.clip(dparam, -5, 5, out=dparam) # clip to mitigate exploding gradients\n",
    "    return loss, dWxh, dWhh, dWhy, dbh, dby, hs[len(inputs)-1]\n",
    "\n",
    "def sample(h, seed_ix, n):\n",
    "    \"\"\" \n",
    "    sample a sequence of integers from the model \n",
    "    h is memory state, seed_ix is seed letter for first time step\n",
    "    \"\"\"\n",
    "    x = np.zeros((vocab_size, 1))\n",
    "    x[seed_ix] = 1\n",
    "    ixes = []\n",
    "    for t in range(n):\n",
    "        h = np.tanh(np.dot(Wxh, x) + np.dot(Whh, h) + bh)\n",
    "        y = np.dot(Why, h) + by\n",
    "        p = np.exp(y) / np.sum(np.exp(y))\n",
    "        ix = np.random.choice(range(vocab_size), p=p.ravel())\n",
    "        x = np.zeros((vocab_size, 1))\n",
    "        x[ix] = 1\n",
    "        ixes.append(ix)\n",
    "    return ixes\n",
    "\n",
    "n, p = 0, 0\n",
    "mWxh, mWhh, mWhy = np.zeros_like(Wxh), np.zeros_like(Whh), np.zeros_like(Why)\n",
    "mbh, mby = np.zeros_like(bh), np.zeros_like(by) # memory variables for Adagrad\n",
    "smooth_loss = -np.log(1.0/vocab_size)*seq_length # loss at iteration \n",
    "\n",
    "while True:\n",
    "    # prepare inputs (we're sweeping from left to right in steps seq_length long)\n",
    "    if p+seq_length+1 >= len(data) or n == 0: \n",
    "        hprev = np.zeros((hidden_size,1)) # reset RNN memory\n",
    "        p = 0 # go from start of data\n",
    "    inputs = [char_to_ix[ch] for ch in data[p:p+seq_length]]\n",
    "    targets = [char_to_ix[ch] for ch in data[p+1:p+seq_length+1]]\n",
    "\n",
    "    # sample from the model now and then\n",
    "    if n % 100 == 0:\n",
    "        sample_ix = sample(hprev, inputs[0], 200)\n",
    "        txt = ''.join(ix_to_char[ix] for ix in sample_ix)\n",
    "        print( '----\\n %s \\n----' % (txt, ))\n",
    "\n",
    "    # forward seq_length characters through the net and fetch gradient\n",
    "    loss, dWxh, dWhh, dWhy, dbh, dby, hprev = lossFun(inputs, targets, hprev)\n",
    "    smooth_loss = smooth_loss * 0.999 + loss * 0.001\n",
    "    if n % 100 == 0: print('iter %d, loss: %f' % (n, smooth_loss) )# print progress\n",
    "\n",
    "    # perform parameter update with Adagrad\n",
    "    for param, dparam, mem in zip([Wxh, Whh, Why, bh, by], \n",
    "                                [dWxh, dWhh, dWhy, dbh, dby], \n",
    "                                [mWxh, mWhh, mWhy, mbh, mby]):\n",
    "        mem += dparam * dparam\n",
    "        param += -learning_rate * dparam / np.sqrt(mem + 1e-8) # adagrad update\n",
    "\n",
    "    p += seq_length # move data pointer\n",
    "    n += 1 # iteration counter \n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
